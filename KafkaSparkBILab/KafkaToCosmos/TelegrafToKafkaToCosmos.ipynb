{"nbformat_minor": 2, "cells": [{"source": "# This notebook takes input from HDInsight Kafka, which in turn takes input from Telegraf, and sends the metrics to Cosmos DB\n\nNote that cosmosdb spark connector must be a uber jar located in HDFS as shown below, the one in Maven repo doesn't have all the dependencies.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure\n{   \n  \"executorCores\": 2, \n  \"driverMemory\" : \"2G\", \n  \"jars\": [\"/path/to/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar\"],\n  \"conf\": {\"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\",\n           \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n          }\n}", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "val kafkaBrokers=\"host1:9092,host2:9092...\"\nval kafkaTopic=\"telegraf\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "/*\n{\n\"fields\":{\n    \"usage_guest\":0,\n    \"usage_guest_nice\":0,\n    \"usage_idle\":97.28643216079983,\n    \"usage_iowait\":1.4070351758792998,\n    \"usage_irq\":0,\n    \"usage_nice\":0,\n    \"usage_softirq\":0,\n    \"usage_steal\":0,\n    \"usage_system\":0.40201005025121833,\n    \"usage_user\":0.9045226130652948},\n\"name\":\"cpu\",\n\"tags\":{\n    \"cpu\":\"cpu0\",\n    \"host\":\"pliukafkawus2\"},\n\"timestamp\":1534985650\n}\n*/\n\nval payloadSchema = new StructType().\n      add(\"fields\", StringType).\n      add(\"name\", StringType).\n      add(\"tags\",StringType).\n      add(\"timestamp\",TimestampType)\n\nval df = spark.\n  readStream.\n  format(\"kafka\").\n  option(\"kafka.bootstrap.servers\", kafkaBrokers).\n  option(\"subscribe\", kafkaTopic).\n  load\n\nval payloaddf = df.\n  select(from_json($\"value\".cast(StringType), payloadSchema).alias(\"payload\")).\n  select($\"payload.timestamp\".cast(StringType).alias(\"ts\"), //throws error if timestamp is not cast to string\n         get_json_object($\"payload.fields\", \"$.usage_idle\").alias(\"usage_idle\"),\n         get_json_object($\"payload.fields\", \"$.usage_iowait\").alias(\"usage_iowait\"),\n         get_json_object($\"payload.fields\", \"$.usage_system\").alias(\"usage_system\"),\n         get_json_object($\"payload.fields\", \"$.usage_user\").alias(\"usage_user\"))\n\n/*\nval query = payloaddf.\n  writeStream.\n  format(\"console\").\n  start\n*/", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import org.joda.time._\nimport org.joda.time.format._\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark.streaming.CosmosDBSinkProvider\nimport com.microsoft.azure.cosmosdb.spark.config.Config", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "val cosmosdbEndpoint = \"https://{cosmosdb_account}.documents.azure.com:443/\"\nval cosmosdbMasterKey = \"{cosmosdb_account_key}\"\nval cosmosdbDatabase = \"metricdb\"\nval cosmosdbCollection = \"metriccollection\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "val configMap = Map(\n  \"Endpoint\" -> cosmosdbEndpoint,\n  \"Masterkey\" -> cosmosdbMasterKey,\n  \"Database\" -> cosmosdbDatabase,\n  \"Collection\" -> cosmosdbCollection)\n\nval query = payloaddf.\n  writeStream.\n  format(classOf[CosmosDBSinkProvider].getName).\n  outputMode(\"append\").\n  options(configMap).\n  option(\"checkpointLocation\", \"/path/to/cosmoscheckpoint\").\n  start\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//for batch instead of streaming, not yet tested\nimport org.apache.spark.sql.{Row, SaveMode, SparkSession}\n\nval writeConfig = Config(configMap)\ndf.write.mode(SaveMode.Overwrite).cosmosDB(writeConfig)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}