{"nbformat_minor": 2, "cells": [{"source": "# This notebook takes input from HDInsight Kafka, which in turn takes input from Telegraf, and sends the metrics to Cosmos DB\n\nNote that cosmosdb spark connector must be a uber jar located in HDFS as shown below, the one in Maven repo doesn't have all the dependencies.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure\n{   \n  \"executorCores\": 2, \n  \"driverMemory\" : \"2G\", \n  \"jars\": [\"/path/to/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar\"],\n  \"conf\": {\"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\",\n           \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n          }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'executorCores': 2, u'jars': [u'/home/pliu/azure-cosmosdb-spark_2.3.0_2.11-1.2.2-uber.jar'], u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}, u'driverMemory': u'2G'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "val kafkaBrokers=\"host1:9092,host2:9092...\"\nval kafkaTopic=\"telegraf\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1534972619938_0007</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-pliusp.p42l3d32serefgaclbznrqafrc.xx.internal.cloudapp.net:8088/proxy/application_1534972619938_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-pliusp.p42l3d32serefgaclbznrqafrc.xx.internal.cloudapp.net:30060/node/containerlogs/container_e02_1534972619938_0007_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nkafkaTopic: String = telegraf"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import spark.implicits._"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "/*\n{\n\"fields\":{\n    \"usage_guest\":0,\n    \"usage_guest_nice\":0,\n    \"usage_idle\":97.28643216079983,\n    \"usage_iowait\":1.4070351758792998,\n    \"usage_irq\":0,\n    \"usage_nice\":0,\n    \"usage_softirq\":0,\n    \"usage_steal\":0,\n    \"usage_system\":0.40201005025121833,\n    \"usage_user\":0.9045226130652948},\n\"name\":\"cpu\",\n\"tags\":{\n    \"cpu\":\"cpu0\",\n    \"host\":\"pliukafkawus2\"},\n\"timestamp\":1534985650\n}\n*/\n\nval payloadSchema = new StructType().\n      add(\"fields\", StringType).\n      add(\"name\", StringType).\n      add(\"tags\",StringType).\n      add(\"timestamp\",TimestampType)\n\nval df = spark.\n  readStream.\n  format(\"kafka\").\n  option(\"kafka.bootstrap.servers\", kafkaBrokers).\n  option(\"subscribe\", kafkaTopic).\n  load\n\nval payloaddf = df.\n  select(from_json($\"value\".cast(StringType), payloadSchema).alias(\"payload\")).\n  select($\"payload.timestamp\".cast(StringType).alias(\"ts\"), //throws error if timestamp is not cast to string\n         get_json_object($\"payload.fields\", \"$.usage_idle\").alias(\"usage_idle\"),\n         get_json_object($\"payload.fields\", \"$.usage_iowait\").alias(\"usage_iowait\"),\n         get_json_object($\"payload.fields\", \"$.usage_system\").alias(\"usage_system\"),\n         get_json_object($\"payload.fields\", \"$.usage_user\").alias(\"usage_user\"))\n\n/*\nval query = payloaddf.\n  writeStream.\n  format(\"console\").\n  start\n*/", "outputs": [{"output_type": "stream", "name": "stdout", "text": "payloaddf: org.apache.spark.sql.DataFrame = [ts: string, usage_idle: string ... 3 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "import org.joda.time._\nimport org.joda.time.format._\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark.streaming.CosmosDBSinkProvider\nimport com.microsoft.azure.cosmosdb.spark.config.Config", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import com.microsoft.azure.cosmosdb.spark.config.Config"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "val cosmosdbEndpoint = \"https://{cosmosdb_account}.documents.azure.com:443/\"\nval cosmosdbMasterKey = \"{cosmosdb_account_key}\"\nval cosmosdbDatabase = \"metricdb\"\nval cosmosdbCollection = \"metriccollection\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "cosmosdbCollection: String = metriccollection"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "val configMap = Map(\n  \"Endpoint\" -> cosmosdbEndpoint,\n  \"Masterkey\" -> cosmosdbMasterKey,\n  \"Database\" -> cosmosdbDatabase,\n  \"Collection\" -> cosmosdbCollection)\n\nval query = payloaddf.\n  writeStream.\n  format(classOf[CosmosDBSinkProvider].getName).\n  outputMode(\"append\").\n  options(configMap).\n  option(\"checkpointLocation\", \"/path/to/cosmoscheckpoint\").\n  start\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4ccf970c"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//for batch instead of streaming, not yet tested\nimport org.apache.spark.sql.{Row, SaveMode, SparkSession}\n\nval writeConfig = Config(configMap)\ndf.write.mode(SaveMode.Overwrite).cosmosDB(writeConfig)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}